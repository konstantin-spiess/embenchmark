{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STS22 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets sentence_transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from  sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import InputExample\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = [\n",
    "  \"all_languages\",\n",
    "  # \"ar\",\n",
    "  # \"de\",\n",
    "  # \"de-en\",\n",
    "  # \"de-fr\",\n",
    "  # \"de-pl\",\n",
    "  # \"en\",\n",
    "  # \"es\",\n",
    "  # \"es-en\",\n",
    "  # \"es-it\",\n",
    "  # \"fr\",\n",
    "  # \"fr-pl\",\n",
    "  # \"it\",\n",
    "  # \"pl\",\n",
    "  # \"pl-en\",\n",
    "  # \"ru\",\n",
    "  # \"tr\",\n",
    "  # \"zh\",\n",
    "  # \"zh-en\"\n",
    "]\n",
    "\n",
    "sts22 = {}\n",
    "\n",
    "for subset in subsets:\n",
    "  sts22[subset] = load_dataset(\"mteb/sts22-crosslingual-sts\", subset, split=\"test\")\n",
    "\n",
    "BINS = [0, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "SUBSETS = subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for subset in SUBSETS:\n",
    "    samples_subset = {}\n",
    "    for idx in range(len(BINS)-1):\n",
    "        key = f\"{BINS[idx]}-{BINS[idx+1]}\"\n",
    "        samples_subset[key] = [InputExample(texts=[item[\"sentence1\"], item[\"sentence2\"]], label=item[\"score\"]/5) for item in sts22[subset] if BINS[idx] <= len(item[\"sentence1\"].split()) < BINS[idx+1]]\n",
    "    samples[subset] = samples_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {}\n",
    "\n",
    "for subset in SUBSETS:\n",
    "    evaluators_subset = {}\n",
    "    samples_subset = samples[subset]\n",
    "    for i in range(len(BINS)-1):\n",
    "        key = f\"{BINS[i]}-{BINS[i+1]}\"\n",
    "\n",
    "        # if there are less than 2 samples, skip this bin\n",
    "        if samples_subset[key] is None or len(samples_subset[key]) < 2:\n",
    "            continue\n",
    "\n",
    "        evaluators_subset[key] = EmbeddingSimilarityEvaluator.from_input_examples(samples_subset[key], name=f\"sts22-{key}\", batch_size=1)\n",
    "    evaluators[subset] = evaluators_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  \"jinaai/jina-embeddings-v2-base-en\",\n",
    "  \"thenlper/gte-base\",\n",
    "  \"intfloat/multilingual-e5-base\"\n",
    "]\n",
    "\n",
    "for model_name in models:\n",
    "  print(\"Evaluating model\", model_name)\n",
    "  model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "  for subset in evaluators:\n",
    "    print(\"Evaluating subset\", subset)\n",
    "    evaluators_subset = evaluators[subset]\n",
    "    for key in evaluators_subset:\n",
    "      print(\"Evaluating bin\", key)\n",
    "      evaluator = evaluators_subset[key]\n",
    "      model_name = model_name.replace(\"/\", \"-\")\n",
    "      output_path = f\"results/sts/sts22-{subset}/{model_name}/\"\n",
    "      os.makedirs(output_path, exist_ok=True)\n",
    "      evaluator(model, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "  for subset in evaluators:\n",
    "    file_paths = []\n",
    "    extracted_data = {'Range': [], 'cosine_spearman': [], 'Sample Count': []}\n",
    "    for key in evaluators[subset]:\n",
    "      model_name = model_name.replace(\"/\", \"_\")\n",
    "      file_path = f\"results/sts/sts22-{subset}/{model_name}/similarity_evaluation_sts22-{key}_results.csv\"\n",
    "      file_paths.append(file_path)\n",
    "\n",
    "      df = pd.read_csv(file_path)\n",
    "      # get last row\n",
    "      cosine_spearman = df['cosine_spearman'].iloc[-1]\n",
    "      extracted_data['Range'].append(key)\n",
    "      extracted_data['cosine_spearman'].append(cosine_spearman)\n",
    "      extracted_data['Sample Count'].append(len(samples[subset][key]))\n",
    "    extracted_df = pd.DataFrame(extracted_data)\n",
    "    print(\"Model\", model_name)\n",
    "    print(\"Subset\", subset)\n",
    "    print(extracted_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

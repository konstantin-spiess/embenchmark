{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mteb==1.1.1 datasets beir sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from mteb.abstasks.AbsTaskRetrieval import DRESModel\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pathlib, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET YOU HUGGINGFACE TOKEN\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CQADupStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DATASET = False\n",
    "\n",
    "if DOWNLOAD_DATASET:\n",
    "  #### Just some code to print debug information to stdout\n",
    "  logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                      datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                      level=logging.INFO,\n",
    "                      handlers=[LoggingHandler()])\n",
    "  #### /print debug information to stdout\n",
    "\n",
    "  #### Download scifact.zip dataset and unzip the dataset\n",
    "  dataset = \"cqadupstack\"\n",
    "  url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "  out_dir = os.path.join(pathlib.Path('.').parent.absolute(), \"datasets\")\n",
    "  data_path = util.download_and_unzip(url, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: Sort Questions into Bins by Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = pd.DataFrame(columns=['model_name', 'subset', 'bin_label', 'NDCG@10', 'count'])\n",
    "\n",
    "\n",
    "subsets = [\n",
    "    \"android\",\n",
    "    \"english\",\n",
    "    \"gaming\",\n",
    "    \"gis\",\n",
    "    \"mathematica\",\n",
    "    \"physics\",\n",
    "    \"programmers\",\n",
    "    \"stats\",\n",
    "    \"tex\",\n",
    "    \"unix\",\n",
    "    \"webmasters\",\n",
    "    \"wordpress\"\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"thenlper/gte-base\",\n",
    "    \"jinaai/jina-embeddings-v2-base-en\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "bin_labels = [f\"{bins[i]}-{bins[i+1]-1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "for model_name in model_names:\n",
    "  print(f\"Benchmarking {model_name}\")\n",
    "  \n",
    "  # Set model and retriever\n",
    "  model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "  model = DRESModel(model)\n",
    "  model = DRES(model, batch_size=1, corpus_chunk_size=50000)\n",
    "  retriever = EvaluateRetrieval(model, score_function=\"cos_sim\")\n",
    "\n",
    "  for subset in subsets:\n",
    "    print(f\"Benchmarking {subset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    corpus, queries, qrels = GenericDataLoader(data_folder=f\"datasets/cqadupstack/{subset}\").load(split=\"test\")\n",
    "\n",
    "    # Create dataframe to calculate word count and assign bins\n",
    "    corpus_df = pd.DataFrame.from_dict(corpus, orient='index')\n",
    "    corpus_df['word_count'] = corpus_df['text'].apply(lambda x: len(x.split()))\n",
    "    corpus_df['bin'] = pd.cut(corpus_df['word_count'], bins=bins, labels=bin_labels)\n",
    "\n",
    "    # Retrieve results\n",
    "    results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "    for i in range (0, len(bins)-1):\n",
    "      bin_start = bins[i]\n",
    "      bin_end = bins[i+1]\n",
    "      bin_label = bin_labels[i]\n",
    "\n",
    "      qrel_keys = []\n",
    "\n",
    "      # Filter query-corpus relations for current bin\n",
    "      for qrel_key, qrel_value in qrels.items():\n",
    "        for corpus_key in qrel_value:\n",
    "          if corpus_df.loc[corpus_key]['bin'] == bin_label:\n",
    "            qrel_keys.append(qrel_key)\n",
    "\n",
    "      bin_qrels = {k: qrels[k] for k in qrel_keys}\n",
    "\n",
    "      # Skip this bin if bin_qrels is empty\n",
    "      if len(bin_qrels) == 0:\n",
    "        continue\n",
    "\n",
    "      # Evaluate results for current bin\n",
    "      ndcg, _map, recall, precision = retriever.evaluate(bin_qrels, results, retriever.k_values)\n",
    "\n",
    "      # Save NDCG@10\n",
    "      entry = {'model_name': model_name, 'subset': subset, 'bin_label': bin_label, 'NDCG@10': ndcg['NDCG@10'], 'count': len(bin_qrels)}\n",
    "      benchmark_results = benchmark_results.append(entry, ignore_index=True)\n",
    "\n",
    "  benchmark_results.to_csv(f'results/retrieval/cqadupstack/benchmark_results_{model_name.replace(\"/\", \"_\")}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

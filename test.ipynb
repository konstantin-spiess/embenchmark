{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load STS datasets\n",
    "sts12 = load_dataset(\"mteb/sts12-sts\")\n",
    "sts13 = load_dataset(\"mteb/sts13-sts\")\n",
    "sts14 = load_dataset(\"mteb/sts14-sts\")\n",
    "sts15 = load_dataset(\"mteb/sts15-sts\")\n",
    "sts16 = load_dataset(\"mteb/sts16-sts\")\n",
    "stsb = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "sickr = load_dataset(\"mteb/sickr-sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets into one\n",
    "sts12_train = sts12[\"train\"]\n",
    "sts12_test = sts12[\"test\"]\n",
    "sts13_test = sts13[\"test\"]\n",
    "sts14_test = sts14[\"test\"]\n",
    "sts15_test = sts15[\"test\"]\n",
    "sts16_test = sts16[\"test\"]\n",
    "stsb_train = stsb[\"train\"]\n",
    "stsb_validation = stsb[\"validation\"]\n",
    "stsb_test = stsb[\"test\"]\n",
    "sickr_test = sickr[\"test\"]\n",
    "\n",
    "dataset = concatenate_datasets([sts12_train, sts12_test, sts13_test, sts14_test, sts15_test, sts16_test, stsb_train, stsb_validation, stsb_test, sickr_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot word count distribution\n",
    "\n",
    "word_count = []\n",
    "for sample in dataset:\n",
    "    word_count.append(len(sample[\"sentence1\"].split()))\n",
    "\n",
    "plt.hist(word_count, bins=[0, 10, 20, 30, 40, 50, 60, 70, 80], edgecolor='black')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "# Create buckets for different word counts and add samples to buckets\n",
    "\n",
    "test_samples = {0:[], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8:[]}\n",
    "for row in dataset:\n",
    "  word_count = len(row[\"sentence1\"].split())\n",
    "  bucket = word_count // 10\n",
    "\n",
    "  # Normalize scores to range 0 ... 1\n",
    "  score = float(row['score']) / 5.0\n",
    "\n",
    "  test_samples[bucket].append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 9):\n",
    "  print(\"Bucket\", i, \":\", len(test_samples[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "#\n",
    "test_evaluator = {}\n",
    "\n",
    "for i in test_samples:\n",
    "  if len(test_samples[i]) > 1:\n",
    "    test_evaluator[i] = EmbeddingSimilarityEvaluator.from_input_examples(test_samples[i], name=str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "models = [\n",
    "  \"all-mpnet-base-v2\",\n",
    "  \"multi-qa-mpnet-base-dot-v1\",\n",
    "  \"all-distilroberta-v1\",\n",
    "  \"all-MiniLM-L12-v2\",\n",
    "  \"multi-qa-distilbert-cos-v1\",\n",
    "  \"all-MiniLM-L6-v2\",\n",
    "  \"multi-qa-MiniLM-L6-cos-v1\",\n",
    "  \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "  \"paraphrase-albert-small-v2\",\n",
    "  \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "  \"paraphrase-MiniLM-L3-v2\",\n",
    "  \"distiluse-base-multilingual-cased-v1\",\n",
    "  \"distiluse-base-multilingual-cased-v2\"\n",
    "]\n",
    "\n",
    "for model_name in models:\n",
    "  print(\"Evaluating\", model_name)\n",
    "  model = SentenceTransformer(model_name)\n",
    "  for i in test_evaluator:\n",
    "    print(\"Bucket\", i, \":\", len(test_samples[i]))\n",
    "    test_evaluator[i](model, output_path=f\"./result/{model_name}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
